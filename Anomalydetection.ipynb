{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9_6alQCT0gQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
       
        "# -------------------------Configuration Settings------------------------------------\n",
        
        "# All tunable parameters are stored here for easy access.\n",
        "CONFIG = {\n",
        "    \"file_path\": \"synthetic_fiscal_metering_data.xlsx\",\n",
        "    \"features\": [\"Flow\", \"Pressure\", \"Temperature\"],\n",
        "    \"timestamp_col\": \"Timestamp\",\n",
        "    \"time_steps\": 60,       # How many past data points the model sees to make a prediction.\n",
        "    \"train_split\": 0.8,\n",
        "    \"lstm_units_1\": 64,\n",
        "    \"lstm_units_2\": 32,\n",
        "    \"dropout_rate\": 0.2,    # Helps prevent overfitting.\n",
        "    \"learning_rate\": 0.0005,\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\": 32,\n",
        "    \"output_dir\": \"Plots\"\n",
        "}\n",
        "\n",
        "# Make sure the output directory exists.\n",
        "if not os.path.exists(CONFIG[\"output_dir\"]):\n",
        "    os.makedirs(CONFIG[\"output_dir\"])\n",
        "\n",
        "# Set a professional plotting style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "\n",
        "# ----------------------Data Handling Functions----------------------------------\n",
        "\n",
        "def load_and_preprocess_data(config):\n",
        "    \"\"\"\n",
        "    Loads the dataset from an Excel file, sets the timestamp as the index,\n",
        "    and scales the features to a range of [0, 1].\n",
        "    \"\"\"\n",
        "    print(\"Loading and preparing data...\")\n",
        "    try:\n",
        "        df = pd.read_excel(config[\"file_path\"])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{config['file_path']}' was not found.\")\n",
        "        return None, None, None\n",
        "\n",
        "    df[config[\"timestamp_col\"]] = pd.to_datetime(df[config[\"timestamp_col\"]])\n",
        "    df = df.set_index(config[\"timestamp_col\"])\n",
        "    df.columns = config[\"features\"]\n",
        "\n",
        "    # Scaling is crucial for neural networks.\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "    print(\"Data successfully loaded and scaled.\")\n",
        "    return scaled_data, scaler, df\n",
        "\n",
        "def create_sequences(data, time_steps):\n",
        "    \"\"\"\n",
        "    Converts the time-series data into time_steps, format required by LSTMs.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        # The sequence of 'time_steps' length\n",
        "        X.append(data[i:(i + time_steps)])\n",
        "        # The value to be predicted\n",
        "        y.append(data[i + time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -----------------Model Architecture-------------------------------------------\n",
        "\n",
        "def build_lstm_model(input_shape, config):\n",
        "    \"\"\"\n",
        "    Defines the stacked LSTM model architecture. We use two LSTM layers for\n",
        "    capturing complex temporal patterns and Dropout for regularization.\n",
        "    \"\"\"\n",
        "    print(\"Building the LSTM model...\")\n",
        "    model = Sequential([\n",
        "        LSTM(config[\"lstm_units_1\"], return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(config[\"dropout_rate\"]),\n",
        "        LSTM(config[\"lstm_units_2\"], return_sequences=False),\n",
        "        Dropout(config[\"dropout_rate\"]),\n",
        "        Dense(input_shape[1]) # The output layer has a neuron for each feature.\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=config[\"learning_rate\"])\n",
        "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "#----------------------------- Anomaly Detection Logic--------------------------\n",
        "\n",
        "def detect_anomalies(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Uses the trained model to predict values on the test set, calculates the\n",
        "    reconstruction error, and identifies anomalies using a statistical threshold.\n",
        "    \"\"\"\n",
        "    print(\"Detecting anomalies on the test data...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # The \"reconstruction error\" is the difference between what the model expected and what actually happened.\n",
        "    errors = np.mean(np.abs(y_test - y_pred), axis=1)\n",
        "\n",
        "    # We'll use a dynamic threshold: mean error + 3 standard deviations.\n",
        "    # This is a common heuristic for outlier detection.\n",
        "    threshold = np.mean(errors) + 3 * np.std(errors)\n",
        "    print(f\"Calculated dynamic anomaly threshold: {threshold:.4f}\")\n",
        "\n",
        "    anomalies_mask = errors > threshold\n",
        "    return y_pred, anomalies_mask, errors, threshold\n",
        "\n",
        "# ----------------------Visualization/Plotting Functions--------------------------------\n",
        "\n",
        "def plot_correlation_matrix(df, config):\n",
        "\n",
        "    print(\"Creating feature correlation heatmap...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    corr_matrix = df.corr()\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Feature Correlation Matrix', fontsize=16, weight='bold')\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], \"00_correlation_matrix.png\"))\n",
        "    plt.close()\n",
        "    print(\"Correlation matrix plot saved.\")\n",
        "\n",
        "def plot_prediction_error_distribution(errors, threshold, config):\n",
        "\n",
        "    print(\"Plotting the distribution of prediction errors...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(errors, bins=50, kde=True, color='navy')\n",
        "    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Anomaly Threshold ({threshold:.4f})')\n",
        "    plt.title('Distribution of Model Prediction Error', fontsize=16, weight='bold')\n",
        "    plt.xlabel('Mean Absolute Error', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], \"prediction_error_distribution.png\"))\n",
        "    plt.close()\n",
        "    print(\"Prediction error distribution plot saved.\")\n",
        "\n",
        "def plot_anomaly_results_parallel(full_df, y_test_rescaled, y_pred_rescaled,\n",
        "                                  mask, feature_index, split_timestamp, config):\n",
        "    \"\"\"\n",
        "    Creates a two-part plot for each feature: one showing the full time-series context\n",
        "    and another on the test set to highlight detected anomalies.\n",
        "    \"\"\"\n",
        "    feature_name = config[\"features\"][feature_index]\n",
        "    units = {\"Flow\": \"Sm³/h\", \"Pressure\": \"bar\", \"Temperature\": \"°C\"}\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
        "    fig.suptitle(f'Analysis for {feature_name}', fontsize=20, weight='bold')\n",
        "\n",
        "    # Top plot: The big picture\n",
        "    ax1.plot(full_df.index, full_df[feature_name], color='navy', label='Full Raw Data')\n",
        "    ax1.axvline(x=split_timestamp, color='red', linestyle='--', linewidth=2, label='Train/Test Split')\n",
        "    ax1.set_title(f'Full Time-Series Data for {feature_name}', fontsize=16)\n",
        "    ax1.set_ylabel(f'{feature_name} ({units.get(feature_name, \"\")})', fontsize=12)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Bottom plot: Zoomed-in view of the test set results\n",
        "    test_indices = full_df.index[-len(y_test_rescaled):]\n",
        "    ax2.plot(test_indices, y_test_rescaled[:, feature_index], label=f'Actual {feature_name}', color='navy', alpha=0.8)\n",
        "    ax2.plot(test_indices, y_pred_rescaled[:, feature_index], label=f'Predicted {feature_name}', color='darkorange', linestyle='--')\n",
        "\n",
        "    # Highlight the anomalies we found\n",
        "    anomalous_indices = np.where(mask)[0]\n",
        "    ax2.scatter(\n",
        "        test_indices[anomalous_indices],\n",
        "        y_test_rescaled[anomalous_indices, feature_index],\n",
        "        color='red', marker='o', s=50, label='Detected Anomaly'\n",
        "    )\n",
        "\n",
        "    ax2.set_title('Anomaly Detection Results on Test Set', fontsize=16)\n",
        "    ax2.set_xlabel('Timestamp', fontsize=12)\n",
        "    ax2.set_ylabel(f'{feature_name} ({units.get(feature_name, \"\")})', fontsize=12)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    output_path = os.path.join(config[\"output_dir\"], f\"04_{feature_name}_analysis.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"Parallel analysis plot for {feature_name} saved.\")\n",
        "\n",
        "\n",
        "# -----------------------Main Execution------------------------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"Runs the full anomaly detection pipeline from start to finish.\"\"\"\n",
        "\n",
        "    # --- 1. Data Loading and Preparation ---\n",
        "    scaled_data, scaler, full_df = load_and_preprocess_data(CONFIG)\n",
        "    if full_df is None:\n",
        "        return # Stop execution if data loading failed\n",
        "\n",
        "    # Let's see how our features are related before we start modeling.\n",
        "    plot_correlation_matrix(full_df, CONFIG)\n",
        "\n",
        "    X, y = create_sequences(scaled_data, CONFIG[\"time_steps\"])\n",
        "    split_idx = int(CONFIG[\"train_split\"] * len(X))\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "    print(f\"Data split into {len(X_train)} training and {len(X_test)} testing samples.\")\n",
        "\n",
        "    # Find the exact timestamp of the split for plotting later.\n",
        "    split_timestamp = full_df.index[split_idx + CONFIG[\"time_steps\"]]\n",
        "\n",
        "    # --- 2. Model Training ---\n",
        "    model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), config=CONFIG)\n",
        "    print(\"\\nKicking off model training. This might take a moment...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=CONFIG[\"epochs\"],\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        validation_split=0.2,\n",
        "        shuffle=False,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"Model training is complete.\")\n",
        "\n",
        "    # --- 3. Anomaly Detection & Performance Evaluation ---\n",
        "    y_pred, anomalies_mask, errors, threshold = detect_anomalies(model, X_test, y_test)\n",
        "\n",
        "    # Calculate Mean Squared Error on the scaled test data.\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Prediction Mean Squared Error (MSE): {mse:.6f}\")\n",
        "\n",
        "    # Since we don't have ground-truth anomaly labels, we'll assume all test points\n",
        "    # are 'normal' to evaluate how many false positives our model generated.\n",
        "    y_true_labels = np.zeros(len(y_test))\n",
        "    y_pred_labels = anomalies_mask.astype(int)\n",
        "\n",
        "\n",
        "    # Visualize the error distribution to see if the threshold makes sense.\n",
        "    plot_prediction_error_distribution(errors, threshold, CONFIG)\n",
        "\n",
        "    # --- 4. Generate Final Result Plots ---\n",
        "    print(\"\\nGenerating final analysis plots for each feature...\")\n",
        "    # Rescale data back to original units for interpretability.\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "\n",
        "    for i in range(len(CONFIG[\"features\"])):\n",
        "        plot_anomaly_results_parallel(\n",
        "            full_df, y_test_rescaled, y_pred_rescaled,\n",
        "            anomalies_mask, feature_index=i,\n",
        "            split_timestamp=split_timestamp, config=CONFIG\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
